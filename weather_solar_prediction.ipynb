{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Cover and Rainfall Prediction for Solar Panel Output in Tamil Nadu\n",
    "\n",
    "This Jupyter notebook demonstrates how to predict cloud cover and rainfall probability in Tamil Nadu, India, to estimate their impact on solar panel energy output. It uses `rioxarray`, `xarray`, `Dask`, and `GDAL` to handle large geospatial datasets efficiently. The notebook converts non-spatial CSV weather data into spatial datasets, performs spatial analysis, and provides visualizations for a demo to a solar panel installation company.\n",
    "\n",
    "## Objectives\n",
    "- Load and process large weather datasets (CSV and raster) using Dask for memory efficiency.\n",
    "- Convert CSV data with coordinates into spatial datasets using `rioxarray`.\n",
    "- Analyze cloud cover and rainfall probability.\n",
    "- Estimate reduced solar panel output due to weather conditions.\n",
    "- Visualize results for a demo in Chennai, Tamil Nadu.\n",
    "\n",
    "## Prerequisites\n",
    "- Install required packages: `pip install rioxarray xarray dask geopandas pandas numpy scikit-learn matplotlib rasterio`\n",
    "- Download sample weather data (e.g., from NASA or IMD) or use synthetic data as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"data/weather\", exist_ok=True)\n",
    "\n",
    "# Initialize Dask client for parallel computing\n",
    "try:\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    print(client)\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize Dask client: {e}\")\n",
    "    print(\"Continuing without Dask distributed computing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Non-Spatial CSV Data\n",
    "\n",
    "We simulate a large CSV dataset containing weather observations (temperature, humidity, cloud cover, precipitation) with latitude and longitude for Tamil Nadu. Dask is used to handle large datasets that may not fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate large CSV weather data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000  # Reduced for demo, use larger for real data\n",
    "data = {\n",
    "    'date': pd.date_range('2024-01-01', periods=n_samples, freq='H'),\n",
    "    'latitude': np.random.uniform(8.0, 13.5, n_samples),  # Tamil Nadu lat range\n",
    "    'longitude': np.random.uniform(77.0, 80.3, n_samples),  # Tamil Nadu lon range\n",
    "    'temperature': np.random.normal(30, 5, n_samples),  # Celsius\n",
    "    'humidity': np.random.uniform(50, 90, n_samples),  # %\n",
    "    'cloud_cover': np.random.uniform(0, 100, n_samples),  # %\n",
    "    'precipitation': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])  # Binary (0: no rain, 1: rain)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "weather_csv_path = 'data/weather/weather_data_tn.csv'\n",
    "df.to_csv(weather_csv_path, index=False)\n",
    "\n",
    "# Load CSV with Pandas (or Dask for very large files)\n",
    "df = pd.read_csv(weather_csv_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert CSV to Spatial Dataset\n",
    "\n",
    "Convert the DataFrame to a GeoDataFrame and then to an `xarray` Dataset with `rioxarray` for spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Create a time, lat, lon xarray grid\n",
    "# For demonstration, we'll create a coarser grid\n",
    "lat_bins = np.linspace(8.0, 13.5, 20)\n",
    "lon_bins = np.linspace(77.0, 80.3, 20)\n",
    "\n",
    "# Use a smaller time sample for the demo (first day only)\n",
    "unique_dates = pd.to_datetime(df.date).dt.date.unique()[:5]\n",
    "time_stamps = pd.to_datetime([f\"{date} 12:00:00\" for date in unique_dates])\n",
    "\n",
    "# Create empty dataset\n",
    "ds = xr.Dataset(\n",
    "    {\n",
    "        'temperature': (['time', 'lat', 'lon'], np.zeros((len(time_stamps), len(lat_bins), len(lon_bins)))),\n",
    "        'humidity': (['time', 'lat', 'lon'], np.zeros((len(time_stamps), len(lat_bins), len(lon_bins)))),\n",
    "        'cloud_cover': (['time', 'lat', 'lon'], np.zeros((len(time_stamps), len(lat_bins), len(lon_bins)))),\n",
    "        'precipitation': (['time', 'lat', 'lon'], np.zeros((len(time_stamps), len(lat_bins), len(lon_bins))))\n",
    "    },\n",
    "    coords={\n",
    "        'time': time_stamps,\n",
    "        'lat': lat_bins,\n",
    "        'lon': lon_bins\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fill dataset with average values from points (basic interpolation)\n",
    "for t, timestamp in enumerate(time_stamps):\n",
    "    date_str = timestamp.strftime('%Y-%m-%d')\n",
    "    day_data = df[pd.to_datetime(df.date).dt.strftime('%Y-%m-%d') == date_str]\n",
    "    \n",
    "    # For each lat-lon grid cell, find nearby points and take their average\n",
    "    for i, lat in enumerate(lat_bins):\n",
    "        for j, lon in enumerate(lon_bins):\n",
    "            # Find points within 0.5 degrees of this grid cell\n",
    "            nearby = day_data[\n",
    "                (np.abs(day_data.latitude - lat) < 0.5) & \n",
    "                (np.abs(day_data.longitude - lon) < 0.5)\n",
    "            ]\n",
    "            \n",
    "            if len(nearby) > 0:\n",
    "                ds['temperature'][t, i, j] = nearby.temperature.mean()\n",
    "                ds['humidity'][t, i, j] = nearby.humidity.mean()\n",
    "                ds['cloud_cover'][t, i, j] = nearby.cloud_cover.mean()\n",
    "                ds['precipitation'][t, i, j] = nearby.precipitation.mean()\n",
    "\n",
    "# Add CRS information\n",
    "ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Synthetic Raster Data\n",
    "\n",
    "Create a synthetic GeoTIFF for cloud cover to demonstrate raster processing. In a real scenario, you would use actual satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic GeoTIFF for cloud cover\n",
    "cloud_tiff_path = 'data/weather/cloud_cover_tn.tiff'\n",
    "\n",
    "# Generate synthetic cloud cover pattern (higher in coastal areas)\n",
    "height, width = 50, 50\n",
    "cloud_data = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "# Create cloud pattern (more clouds in eastern coastal areas)\n",
    "for i in range(width):\n",
    "    for j in range(height):\n",
    "        # Distance from east coast (normalized)\n",
    "        east_distance = 1 - (i / width)\n",
    "        # More clouds near coast\n",
    "        cloud_data[j, i] = 30 + 60 * east_distance + np.random.normal(0, 10)\n",
    "\n",
    "# Clip values to valid range\n",
    "cloud_data = np.clip(cloud_data, 0, 100)\n",
    "\n",
    "# Write to GeoTIFF\n",
    "with rasterio.open(\n",
    "    cloud_tiff_path, 'w', driver='GTiff',\n",
    "    height=height, width=width, count=1, dtype='float32',\n",
    "    crs='EPSG:4326',\n",
    "    transform=rasterio.transform.from_bounds(77.0, 8.0, 80.3, 13.5, width, height)\n",
    ") as dst:\n",
    "    dst.write(cloud_data, 1)\n",
    "\n",
    "# Load raster with rioxarray\n",
    "cloud_raster = rioxarray.open_rasterio(cloud_tiff_path)\n",
    "print(cloud_raster)\n",
    "\n",
    "# Plot cloud cover raster\n",
    "plt.figure(figsize=(10, 8))\n",
    "cloud_raster.plot(cmap='Blues')\n",
    "plt.title('Cloud Cover Raster (%) - Tamil Nadu')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Predict Rainfall Probability\n",
    "\n",
    "Train a Random Forest model to predict rainfall probability using the simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "X = df[['temperature', 'humidity', 'cloud_cover']]\n",
    "y_rain = df['precipitation']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_rain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "print(f\"Rainfall Prediction Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.title('Feature Importance for Rainfall Prediction')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Predict Rainfall Probability Across Tamil Nadu\n",
    "\n",
    "Apply the trained model to grid data to create a rainfall probability map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict rain probability for a given grid\n",
    "def predict_rain_probability(ds, time_idx):\n",
    "    # Extract features for the specific time\n",
    "    temp = ds['temperature'].isel(time=time_idx).values.flatten()\n",
    "    humidity = ds['humidity'].isel(time=time_idx).values.flatten()\n",
    "    cloud = ds['cloud_cover'].isel(time=time_idx).values.flatten()\n",
    "    \n",
    "    # Create feature array for prediction\n",
    "    features = np.column_stack([temp, humidity, cloud])\n",
    "    \n",
    "    # Remove missing values\n",
    "    valid_idx = ~np.isnan(features).any(axis=1)\n",
    "    valid_features = features[valid_idx]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    if len(valid_features) > 0:\n",
    "        probas = rf.predict_proba(valid_features)[:, 1]  # Probability of class 1 (rain)\n",
    "        \n",
    "        # Create result array (initialize with NaN)\n",
    "        result = np.full(temp.shape, np.nan)\n",
    "        result[valid_idx] = probas  # Assign values to valid indices\n",
    "        \n",
    "        # Reshape back to grid dimensions\n",
    "        return result.reshape(ds['temperature'].isel(time=time_idx).shape)\n",
    "    else:\n",
    "        return np.full_like(temp.reshape(ds['temperature'].isel(time=time_idx).shape), np.nan)\n",
    "\n",
    "# Calculate rain probability for each time step\n",
    "rain_probas = []\n",
    "for t in range(len(ds.time)):\n",
    "    rain_probas.append(predict_rain_probability(ds, t))\n",
    "\n",
    "# Add to dataset\n",
    "ds['rain_probability'] = (['time', 'lat', 'lon'], np.stack(rain_probas))\n",
    "\n",
    "# Plot rainfall probability for the first time step\n",
    "plt.figure(figsize=(10, 8))\n",
    "ds['rain_probability'].isel(time=0).plot(cmap='Blues', vmin=0, vmax=1)\n",
    "plt.title(f'Rainfall Probability - {ds.time.values[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Estimate Solar Panel Output Reduction\n",
    "\n",
    "Estimate the reduction in solar panel output based on cloud cover and rainfall. Assume a linear reduction model: output reduces by 0.5% per 1% cloud cover, and rain reduces output by an additional 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar output reduction model\n",
    "def calculate_solar_output(cloud_cover, rain_prob):\n",
    "    base_output = 100  # Max output in clear conditions (%)\n",
    "    cloud_reduction = cloud_cover * 0.5  # 0.5% reduction per 1% cloud cover\n",
    "    rain_reduction = np.where(rain_prob > 0.5, 20, 0)  # 20% reduction if rain probability > 50%\n",
    "    return base_output - cloud_reduction - rain_reduction\n",
    "\n",
    "# Apply to dataset\n",
    "ds['solar_output'] = calculate_solar_output(ds['cloud_cover'], ds['rain_probability'])\n",
    "\n",
    "# Plot solar output for the first time step\n",
    "plt.figure(figsize=(10, 8))\n",
    "ds['solar_output'].isel(time=0).plot(cmap='viridis', vmin=0, vmax=100)\n",
    "plt.title(f'Estimated Solar Panel Output (%) - {ds.time.values[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results for Chennai\n",
    "\n",
    "Create maps and plots for Chennai to demonstrate weather impacts on solar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for Chennai (approx. lat: 13.08, lon: 80.27)\n",
    "# Find nearest grid point to Chennai\n",
    "chennai_lat, chennai_lon = 13.08, 80.27\n",
    "chennai_ds = ds.sel(lat=chennai_lat, lon=chennai_lon, method='nearest')\n",
    "\n",
    "# Plot time series for Chennai\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# Plot cloud cover\n",
    "chennai_ds['cloud_cover'].plot(ax=axes[0], marker='o')\n",
    "axes[0].set_title('Cloud Cover in Chennai')\n",
    "axes[0].set_ylabel('Cloud Cover (%)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot rain probability\n",
    "chennai_ds['rain_probability'].plot(ax=axes[1], marker='o', color='blue')\n",
    "axes[1].set_title('Rain Probability in Chennai')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot solar output\n",
    "chennai_ds['solar_output'].plot(ax=axes[2], marker='o', color='green')\n",
    "axes[2].set_title('Estimated Solar Output in Chennai')\n",
    "axes[2].set_ylabel('Output (%)')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results for Demo\n",
    "\n",
    "Save the processed dataset and visualizations for the solar panel company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to NetCDF file\n",
    "output_path = 'data/weather/solar_weather_tn.nc'\n",
    "ds.to_netcdf(output_path)\n",
    "print(f\"Dataset saved to {output_path}\")\n",
    "\n",
    "# Save key plots\n",
    "plt.figure(figsize=(10, 8))\n",
    "ds['solar_output'].isel(time=0).plot(cmap='viridis', vmin=0, vmax=100)\n",
    "plt.title(f'Estimated Solar Panel Output (%) - {ds.time.values[0]}')\n",
    "plt.savefig('data/weather/solar_output_map.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Save time series for Chennai\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "chennai_ds['cloud_cover'].plot(ax=axes[0], marker='o')\n",
    "axes[0].set_title('Cloud Cover in Chennai')\n",
    "chennai_ds['solar_output'].plot(ax=axes[1], marker='o', color='green')\n",
    "axes[1].set_title('Estimated Solar Output in Chennai')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/weather/chennai_solar_output.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Plots saved to data/weather/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to process weather datasets, convert CSV data to spatial datasets, predict rainfall, and estimate solar panel output reductions. The visualizations and saved outputs are ready for a demo to a solar panel installation company in Tamil Nadu. For real-world use, replace synthetic data with actual weather data from sources like the India Meteorological Department (IMD) or NASA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}